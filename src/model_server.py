# create a local llm server using llama cpp python
#
